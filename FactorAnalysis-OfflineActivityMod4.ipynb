{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7c57cc",
   "metadata": {},
   "source": [
    "# Factor Analysis\n",
    "**Raphael Kreft** - 23.05.2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a79b1a1",
   "metadata": {},
   "source": [
    "In this notebook, we will discover a technology, widely used across various areas such as Machine Learning, Data Analysis and Drug Discovery. During our exploration of Factor-Analysis we will use the following questions as guidelines:\n",
    "\n",
    "- What is factor analysis?\n",
    "- What are the relationships between covariance matrix, factor analysis, and principal component analysis (PCA)?\n",
    "- What do we mean with loadings?\n",
    "- Why factors are orthogonal to each other? What is the consequence?\n",
    "- How can we use factor analysis as a generative model?\n",
    "- What is the relationship between factor analysis and autoencoder?\n",
    "- How can you it explain factor analysis to a high-school student?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78150153",
   "metadata": {},
   "source": [
    "## What is Factor Analysis\n",
    "\n",
    "Factor Analysis originally was invented to analyse intelligence tests. Based on the results of an IQ Test, the psychologist Charles Spearman deduced, that a good portion of the Test results was explainable by just one personal factor, the so-called g-factor(general factor). The Idea of one or multiple factors that are accurately explaining observed data(but not in the data explicitly) was further developed and became an important technique in the field of descriptive statistics where we consider the following setting:\n",
    "\n",
    "Consider a Dataset $X$ containing samples $x_i\\; i=1..N$ where $N$ is the size of the Dataset. The Dataset has different \"Columns\" that model a specific piece of Information, for example temperature, height or price. Each sample is associated with values for each of the columns. You can imagine a Dataset as a table, where each row contains one sample, and each column contains the values for a specific variable. The following example shows an example dataset about penguins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b2cbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>46.8</td>\n",
       "      <td>14.3</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4850.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>50.4</td>\n",
       "      <td>15.7</td>\n",
       "      <td>222.0</td>\n",
       "      <td>5750.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>45.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>212.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>49.9</td>\n",
       "      <td>16.1</td>\n",
       "      <td>213.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0    Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1    Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2    Adelie  Torgersen            40.3           18.0              195.0   \n",
       "3    Adelie  Torgersen             NaN            NaN                NaN   \n",
       "4    Adelie  Torgersen            36.7           19.3              193.0   \n",
       "..      ...        ...             ...            ...                ...   \n",
       "339  Gentoo     Biscoe             NaN            NaN                NaN   \n",
       "340  Gentoo     Biscoe            46.8           14.3              215.0   \n",
       "341  Gentoo     Biscoe            50.4           15.7              222.0   \n",
       "342  Gentoo     Biscoe            45.2           14.8              212.0   \n",
       "343  Gentoo     Biscoe            49.9           16.1              213.0   \n",
       "\n",
       "     body_mass_g     sex  \n",
       "0         3750.0    Male  \n",
       "1         3800.0  Female  \n",
       "2         3250.0  Female  \n",
       "3            NaN     NaN  \n",
       "4         3450.0  Female  \n",
       "..           ...     ...  \n",
       "339          NaN     NaN  \n",
       "340       4850.0  Female  \n",
       "341       5750.0    Male  \n",
       "342       5200.0  Female  \n",
       "343       5400.0    Male  \n",
       "\n",
       "[344 rows x 7 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "dataset = sns.load_dataset('penguins')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f9bf2",
   "metadata": {},
   "source": [
    "The entries of one column can be interpreted as samples from an underlying random variable.\n",
    "\n",
    "As mentioned above, the goal of factor analysis is to find hidden factors for explicitly observed data. We assume the existance of a set of hidden variables that explains the observed correlation and interrelations between the observed variables.\n",
    "\n",
    "**An example:** Imagine you are a big company that serves online content streaming. Like all other hip tech companies you want to have a recommendation system that recommends users what video they could watch next. For this sake, you are given a Dataset containing the name of people and a movie they watched when. Factor analysis can then be used to determine and isolate factors that predispose a person to like a specific kind of video. A question one can ask is: Are the weekday, time and age all good measures for the watchtime a person has per week?\n",
    "\n",
    "### General Approach to factor Analysis\n",
    "\n",
    "In general, Factor Analysis consists of two steps: \n",
    "1. The **Factor extraction** and \n",
    "2. the **Factor Rotation**. \n",
    "\n",
    "Where the goal of factor extraction is to find the latent factors, Factor Rotation pursues a better interpretability of factors by simplifying their structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3976b197",
   "metadata": {},
   "source": [
    "## What are the relationships between covariance matrix, factor analysis and PCA?\n",
    "\n",
    "The covariance of two variables X, Y describes how much two variables vary together: $$\\sigma(x, y) = \\frac{1}{n-1}\\sum_i=1^n (x_i -x_{mean})(y_i-y_{mean})$$.\n",
    "\n",
    "A **Covariance Matrix** $C^{d\\dot d}$ where d is the number of variables/columns in a dataset, contains the pairwise covariance between all Variables. For a Dataset with two Variables, X and Y the Covariance Matrix looks like that: \n",
    "$$C = \\left[ {\\begin{array}{cc}\n",
    "    \\sigma(x,x) & \\sigma(x,y) \\\\\n",
    "    \\sigma(y,x) & \\sigma(y,y) \\\\\n",
    "  \\end{array} } \\right]$$\n",
    "\n",
    "Exactly these correlations describe the interrelationships between the variables. Factor analysis aims at finding (fewer) latent variables/factors that can explain and model these correlations / interrelationships. \n",
    "\n",
    "There are **different Methods to tackle factor extraction, which differ in the assumptions made about variance**. Starting with a Total Variance of a Dataset, We differentiate between Common Variance and Unique Variance.\n",
    "1. Common variance: Is a Variance that is shared by a set of Variables\n",
    "2. Unique variance: Is the variance inherent to one variable. Here we further differentiate between Error and specific variance\n",
    "![Diagram Showing Vriance Types](https://stats.oarc.ucla.edu/wp-content/uploads/2018/05/fig02d.png)\n",
    "\n",
    "One Method to find factors is **Principal Component Analysis(PCA)**. PCA is a famous unsupervised Machine Learning Technique that is often reffered to as dimensionality reduction technique. PCA will output the directions of maximal variance in the dataset, also called Principal Components. The Principal Components are then the latent factors. PCA assumes that there is just a common Variance and never bothers about Unique variance.\n",
    "\n",
    "Another technique that also accounts for Unique Variance is **Common Factor Analysis**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a4595",
   "metadata": {},
   "source": [
    "## What do we mean with loadings?\n",
    "\n",
    "A factor loading, also called factor score **measures how strong a connection between a specific factor and an observed variable is**. It correlates a latent factor with a common observed variable. If a factor score is high, we know that it has a strong correlation between itself and the specific variable for that the factor score is denoted.\n",
    "\n",
    "Loadings are an important measure that allows to interpret the results of factor analysis. To pick up the example from above: Having a high loading between the observed variable watchtime and a latent factor, we can be quite sure that this factor heavily influences/determines the outcome of the observed variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f4fa13",
   "metadata": {},
   "source": [
    "## Why are factors orthogonal to each other? What is the consequence?\n",
    "\n",
    "When a factor would not be orthogonal to each other, it would mean that there would be a correlation between them. Thus the factors would themselves again have a factor. As we consider the factors to be independent of each other we need them to be orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956f459",
   "metadata": {},
   "source": [
    "## How can we use factor analysis as a generative model ? \n",
    "\n",
    "Factor Analysis aims at finding a set of factors that can model the explicitly observed variables and samples in a dataset.\n",
    "After obtaining a set of factors, we can use a linear combination of those to generate arbitrary samples. In case of PCA, we can use a linear combination of Principal Components to either reconstruct samples from the dataset or generate new samples by choosing individual linear factors.\n",
    "\n",
    "In Linear-Algebra terms, the Principal Components form an orthogonal Basis of a vector-space which the data lives in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3f4a7f",
   "metadata": {},
   "source": [
    "## What is the relationship between factor analysis and autoencoders?\n",
    "\n",
    "Autoencoders are a type of Neural Network Architecture where the goal is to learn a representation of data. They consist of 3 Parts: The encoder, the latent space representation and the decoder. \n",
    "\n",
    "To train the network, it gets fed with data samples ex images. The encoder compresses the input into a latent representation. From this representation the decoder tries to reconstruct the original input. With more samples and time, the network gets better and better and the reconstructed output will cerainly look like the original input.\n",
    "\n",
    "Autoencoders are applicaple in a variety of areas, such as compression Algorithms or generative models such as Github Copilot.\n",
    "\n",
    "![Autoencoder Architecture](https://www.compthree.com/images/blog/ae/ae.png)\n",
    "\n",
    "Similar to factor analysis, autoencoders learn a latent representation. Autoencoders learn efficient encodings to represent high dimensional data by a low dimensional space while losing as few information as possible. They are superior to PCA when they work with highly non-linear and complex data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c14512b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pids",
   "language": "python",
   "name": "pids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
